\documentclass{article}
\usepackage{geometry}
\geometry{tmargin=1in,bmargin=1in,lmargin=1in,rmargin=1in}

<<setup, include=FALSE>>=
library(knitr)
set.seed(0)
@

\begin{document}
\title{Stat243: Problem Set 8 \\ \small Worked with Jamie Palumbo, Mingyung Kim, Alanna Iverson}
\author{Sicun Huang}
\date{December 4, 2015}

\maketitle

\noindent 1. Conduct a simulation study on how regression methods perform when there are outlying values by comparing a new method that is supposedly robust to outliers to standard linear regression. 
\\[12pt]
(a) The basic steps of the simulation study:
\\[12pt]
(1) Specify what makes up an individual experiment: 
\\[12pt]
(i) Sample size: To make results from both methods comparable, use the same sample size when conducting the tests. Choose sample size to be greater than 100 so that prediction is significant.   
\\[12pt]
(ii) Distributions: Choose a distribution with heavy tails so that there is a high chance of getting outliers. Here, we choose to generate data from a normal distribution with a large variance. Then, purpose half of the data set for prediction and the other half for estimation so that we can test the accuracy of the simulation study.
\\[12pt]
(iii) Parameters: We choose to use the same parameters for both methods, the intercept and the slope. 
\\[12pt]
(iv) Statistics of interest: We are interested in the absolute prediction error and coverage of prediction intervals for new observations, where the prediction intervals are based on using the nonparametric bootstrap.
\\[12pt]
(v) Bootstrap size: Choose large enough bootstrap size so that prediction is significant.  
\\[12pt]
(vi) Significance level: Conduct test at different significance levels (e.g. 0.10, 0.05, 0.01).
\\[12pt]
(2) Determine what inputs to vary:
\\[12pt]
(i) Sample size: Start with a relatively small number, then increase it gradually to see its effect on absolute prediction error and coverage of prediction intervals.
\\[12pt]
(ii) Significance level: Vary significance level to see its effect on prediction interval.
\\[12pt]
(iii) Bootstrap size: Similar to sample size, start with a relatively small number, then increase it gradually to see its effect on absolute prediction error and coverage of prediction intervals. 
\\[12pt]
(3) Write code to carry out the individual experiment and return the quantities of interest:
\\[12pt]
We loop over each combination of the sample sizes, bootstrap sizes and significance levels and return the absolute prediction error and coverage of prediction intervals. 
\\[12pt]
(4) For each combination of inputs, repeat the experiment m times.
\\[12pt]
(5) Summarize the results for each combo of interest, quantifying the simulation uncertainty.
\\[12pt]
(6) Report the results in graphical or tabular form.
\\[12pt]
(b) Now we demonstrate the above process with an example:
\\[12pt]
We begin with generating 4000 $x_i$'s and $y_i$'s from normal distributions. Choose a high variance to get outliers; this will allow us to test the robustness of the method. Then, divide the data set into two halves, where one will be used for prediction and the other for estimation. Run a linear regression on the estimation half to get estimates of the $\beta$'s. Then, run the prediction function (predict) with these estimated $\beta$'s to get a set of new $y_i$'s. Compare the new $y_i$'s to $y_i$'s in the prediction data set (values we believe the $y_i$'s should be). We can then calculate the absolute prediction error and the coverage of the prediction interval using these results.
\\[12pt]    
\noindent 2.
\\[12pt]
(a) By looking at the density functions of Pareto distribution and exponential distribution, we can see that tail of the Pareto decays more slowly than that of an exponential distribution. Since the sampling density (Pareto) have heavier tails than the density of interest (exponential), we can use it with the exponential distribution for importance sampling.
\\[12pt]
(b) 

<<r-chunk1, tidy=TRUE>>=
#note: use PtProcess package here because other packages use general pareto distribution
library(PtProcess)

#number of samples for each estimator
m <- 1000 

set.seed(0)
#samples for importance sampler
#sample from g(x), pdf of Pareto(2,3)
x <- rpareto(m, lambda=3, a=2) 
#density of x under f; shifted to the right by 2
f <- dexp(x-2, rate = 1) 
#density of x under g
g <- dpareto(x, lambda=3, a=2)  
#weights
w <- f/g  

est1 <- mean(w*x) 
est1
est2 <- mean(w*(x^2)) 
est2

#h(x)f(x)/g(x) with h(x)=x
estX <- w*x 
#h(x)f(x)/g(x) with h(x)=x^2
estX2 <- w*(x^2)  

par(mfrow = c(2, 2), pty = "s")  
hist(estX) 
hist(estX2) 
hist(w)

var(x)
var(estX)
var(x^2)
var(estX2)
@

Since we chose the distribution with heavier tails (Pareto) as g, the weight of importance sampling, $w=f(x)/g(x)$, is large only when h(x) is very small; this will help avoid having overly influential points. So Var(h(x)f(x)/g(x)), which is proportional to $Var(\hat{\mu})$, is smaller than or similar to Var(h(x)); Var(h(x)f(x)/g(x)) is smaller than Var(h(x)) especially when $h(x)=x^2$, since such a h can produce more extreme values comparing to when $h(x)=x$. 
\\[12pt]
(c)

<<r-chunk2, tidy=TRUE>>=
rm(list=ls())

#number of samples for each estimator
m <- 1000 

set.seed(0)
#samples for importance sampler
#sample from g(x), pdf of exp(1)+2
x <- rexp(m, rate = 1) + 2 
#density of x under f
f <- dpareto(x, lambda=3, a=2)  
#density of x under g
g <- dexp(x-2, rate = 1) 
#weights
w <- f/g  

est1 <- mean(w*x) 
est1
est2 <- mean(w*(x^2)) 
est2

#h(x)f(x)/g(x) with h(x)=x
estX <- w*x 
#h(x)f(x)/g(x) with h(x)=x^2
estX2 <- w*(x^2)

par(mfrow = c(2, 2), pty = "s")  
hist(estX) 
hist(estX2) 
hist(w)

var(x)
var(estX)
var(x^2)
var(estX2)
@

Since we chose the distribution with heavier tails (Pareto) as f, the weight of importance sampling, $w=f(x)/g(x)$, is large even when h(x) is not small; this will result in overly influential points. So Var(h(x)f(x)/g(x)), which is proportional to $Var(\hat{\mu})$, is larger than Var(h(x)); especially when $h(x)=x^2$, since such a h can produce more extreme values comparing to when $h(x)=x$.
\newpage
\noindent 3.
\\[12pt]
(b) Since we only know the observed $y_i$'s, the reasonable starting value for $\beta$ is the estimated $\beta$ from the lm() function. i.e. to make a guesses of the starting value of $\beta$, we ignore the missing value $z_i$'s.
\\[12pt]
(c) Write an R function to estimate the parameters ($\beta_0$, $\beta_1$, $\beta_2$, $\beta_3$). We choose to stop the optimization if error is less than tolerance value or if number of itations exceeds 10000. Let $n=100$; choose parameters that satisfy $\hat{\beta_1}/se(\hat{\beta_1}) \approx 2$ and $\beta_2=\beta_3=0$ (i.e. choose $\beta_1$ such that the signal to noise ratio in the relationship between $x_1$ and y is moderately large).

<<r-chunk3, tidy=FALSE>>=
#function to solve for EM estimators in probit regression
probitEM <- function(y, x1, x2, x3, intVal=c(1,0.5,0,0), tolerance = .Machine$double.eps^0.5, 
                     maxIteration=10000){   
  n <- length(y)
  #intialization
  b0 <- intVal[1]
  b1 <- intVal[2]
  b2 <- intVal[3]
  b3 <- intVal[4]
  it <- 1
  diff <- Inf
  
  while (diff > tolerance & it < maxIteration){ 
    #save starting values of beta's for later
    intb0 <- b0
    intb1 <- b1
    intb2 <- b2
    intb3 <- b3
    
    #E step
    mu <- b0 + b1*x1 + b2*x2 + b3*x3
    #from formula in (a)
    #notice z is N(mu,1), latent variable 
    z <- ifelse(y==1, mu+dnorm(mu,mean=0,sd=1)/pnorm(mu,mean=0,sd=1), 
                mu-dnorm(mu,mean=0,sd=1)/pnorm(-mu,mean=0,sd=1))    

    #M step; estimate betas using lm function     
    b0 <- coef(lm(z ~ x1+x2+x3))[1]
    b1 <- coef(lm(z ~ x1+x2+x3))[2]
    b2 <- coef(lm(z ~ x1+x2+x3))[3]
    b3 <- coef(lm(z ~ x1+x2+x3))[4]
    
    absDiff <- abs(c(b0-intb0, b1-intb1, b2-intb2, b3-intb3))
    diff <- sum(absDiff)/sum(abs(c(intb0, intb1, intb2, intb3)))
    
    it <- it+1
  }
  return(list(b0, b1, b2, b3, it))
}

#function to test different beta values to find the ones that make b1hat/se(b1hat) close to 2
test <- function(b0,b1){
  #assumed in problem
  n <- 100
  b2 <- 0
  b3 <- 0
  x1 <- rnorm(n)
  x2 <- rnorm(n)
  x3 <- rnorm(n)
  XTb <- b0 + b1*x1 + b2*x2 + b3*x3
  
  set.seed(0)
  y <- rbinom(n, 1, prob = pnorm(XTb))
  
  #return z value for coefficient of x1, check if close to 2
  summary(glm(y ~ x1+x2+x3, family=binomial(link = "probit")))$coef[2,3]
}

test(0,0)
test(1,1)
test(1,0.5)
#close to 2
test(1,0.3)

#results from the above trials
b0 <- 1
b1 <- 0.3

#assumed in problem
n <- 100
b2 <- 0
b3 <- 0
x1 <- rnorm(n)
x2 <- rnorm(n)
x3 <- rnorm(n)
XTb <- b0 + b1*x1 + b2*x2 + b3*x3

set.seed(0)
y <- rbinom(n, 1, prob = pnorm(XTb))

#test choice of starting value of beta in (b)
linearReg <- lm(y ~ x1 + x2 + x3)
intVal <- as.double(coef(linearReg))
probitEM(y, x1, x2, x3, intVal)
@

\noindent(d) As an alternative to (c), we can directly maximize the log-likelihood of the observed data. Here we estimate the parameters and standard errors using optim() with the BFGS option; compare iterations that EM and BFGS took respectively.

<<r-chunk4, tidy=FALSE>>=
#function to derive log-likelihood
probitLoglik <- function(beta, X, y){
  #result from part (a)
  p <- pnorm(X%*%beta, mean=0, sd=1, lower.tail = TRUE, log.p = FALSE)
  #log-likelihood of bernoulli
  return(sum(y*log(p)+(1-y)*log(1-p)))
}

#use the same setup as part (c)
n <- 100
b2 <- 0
b3 <- 0
b0 <- 1
b1 <- 0.3
x1 <- rnorm(n)
x2 <- rnorm(n)
x3 <- rnorm(n)
X <- cbind(1,x1,x2,x3)
XTb <- b0 + b1*x1 + b2*x2 + b3*x3

set.seed(0)
y <- rbinom(n, 1, prob = pnorm(XTb))

linearReg <- lm(y ~ x1 + x2 + x3)
intVal <- as.double(coef(linearReg))

#estimate betas
#trace: print iterations; maxit: maximum iterations; fnscale=-1: flip log-likelihood function 
#to maximize
result <- optim(intVal, fn=probitLoglik, gr=NULL, y=y, X=X, method="BFGS", 
                control=list(trace=TRUE,maxit=10000,fnscale=-1), hessian=TRUE)
print(result)

#compute standard error
se <- sqrt((-1)*diag(solve(result$hessian)))
print(se)
@

Observe that the BFGS method took 29 iterations while the EM method took 40.
\\[12pt]
\noindent 4. Plot slices of the “helical valley” function at -10, 0, 5, and 10 to get a sense for how it behaves. Explore the possibility of multiple local minima by using different starting points (0,0,0), (-1,-4,7), and (1,2,5).

<<r-chunk5, cashe=TRUE, tidy=TRUE>>=
library(fields);

#helical valley function
theta <- function(x1,x2) atan2(x2, x1)/(2*pi)

f <- function(x) {
  f1 <- 10*(x[3] - 10*theta(x[1],x[2]))
  f2 <- 10*(sqrt(x[1]^2+x[2]^2)-1)
  f3 <- x[3]
  return(f1^2+f2^2+f3^2)
}

par(mfrow = c(2,2), pty = 's')

x <- seq(-10, 10, length.out=50)
y <- seq(-10, 10, length.out=50)

#heat map and contour lines at z = -10
val <- apply(as.matrix(expand.grid(x, y)), 1, function(x) f(c(x, -10)))
image(x, y, matrix(val, 50, 50), col = heat.colors(100), axes = TRUE, main="Helical at z=-10")
contour(x, y, matrix(val, 50, 50), add = TRUE)

#heat map and contour lines at z = 0
val <- apply(as.matrix(expand.grid(x, y)), 1, function(x) f(c(x, 0)))
image(x, y, matrix(val, 50, 50), col = heat.colors(100), axes = TRUE, main="Helical at z=0")
contour(x, y, matrix(val, 50, 50), add = TRUE)

#heat map and contour lines at z = 5
val <- apply(as.matrix(expand.grid(x, y)), 1, function(x) f(c(x, 5)))
image(x, y, matrix(val, 50, 50), col = heat.colors(100), axes = TRUE, main="Helical at z=5")
contour(x, y, matrix(val, 50, 50), add = TRUE)

#heat map at and contour lines z = 10
val <- apply(as.matrix(expand.grid(x, y)), 1, function(x) f(c(x, 10)))
image(x, y, matrix(val, 50, 50), col = heat.colors(100), axes = TRUE, main="Helical at z=10")
contour(x, y, matrix(val, 50, 50), add = TRUE)

#check that the optimal value changes with starting point
optim(c(0,0,0), f, hessian = TRUE)
optim(c(-1,-4,7), f, hessian = TRUE)
optim(c(1,2,5), f, hessian = TRUE)

nlm(f, c(0,0,0), hessian = TRUE)
nlm(f, c(-1,-4,7), hessian = TRUE)
nlm(f, c(1,2,5), hessian = TRUE)
@

\end{document}